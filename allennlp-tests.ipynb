{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097cf9b2-c835-4d63-93e6-fb98636763f9",
   "metadata": {},
   "source": [
    "# AllenNLP tests\n",
    "\n",
    "Use AllenNLP's semantic role labeling in combination with manually written rules for extracting causal relations from political speeches. We found two limitations: only relations within a single sentence were found and complex relations involving understanding the sentence were hard to identify.\n",
    "\n",
    "Links:\n",
    "\n",
    "* software installation: https://github.com/allenai/allennlp (do not forget to install NLTK popular models)\n",
    "* software usage: https://demo.allennlp.org/semantic-role-labeling (tab: Model Usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc310b0-1c94-4561-b41d-b76cfd4a48a5",
   "metadata": {},
   "source": [
    "## 1. Language processing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4d59808-a18f-4dfe-970a-35ac0d46f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ed53c-b3e5-4cf0-bd00-023d00367908",
   "metadata": {},
   "source": [
    "We use AllenNLP for semantic role labeling and Spacy for lemmatization (of verbs) and tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "403ac43b-17a2-41ca-8826-7651f5f3105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_analyze = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289eae14-db61-4df0-baec-41b07319a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erikt/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/tango/__init__.py:18: UserWarning: AllenNLP Tango is an experimental API and parts of it might change or disappear every time we release a new version.\n",
      "  \"AllenNLP Tango is an experimental API and parts of it might change or disappear \"\n",
      "2021-12-07 13:15:37,253 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
      "2021-12-07 13:15:37,559 - INFO - cached_path - cache of https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz is up-to-date\n",
      "2021-12-07 13:15:37,560 - INFO - allennlp.models.archival - loading archive file https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz from cache at /home/erikt/.allennlp/cache/b5f1db011cc85691a5fa2bf29e055a712261a2e5d74a74edd7da2fffc98d4ab8.4c4ac7e06ec3d85631bd26b839f90b5a375d3ceeb43e3c74f1cf4758dcee2bb3\n",
      "2021-12-07 13:15:37,568 - INFO - allennlp.models.archival - extracting archive file /home/erikt/.allennlp/cache/b5f1db011cc85691a5fa2bf29e055a712261a2e5d74a74edd7da2fffc98d4ab8.4c4ac7e06ec3d85631bd26b839f90b5a375d3ceeb43e3c74f1cf4758dcee2bb3 to temp dir /tmp/tmpewei2y34\n",
      "2021-12-07 13:15:43,914 - INFO - allennlp.common.params - dataset_reader.type = srl\n",
      "2021-12-07 13:15:43,917 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
      "2021-12-07 13:15:43,917 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
      "2021-12-07 13:15:43,920 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
      "2021-12-07 13:15:43,921 - INFO - allennlp.common.params - dataset_reader.token_indexers = None\n",
      "2021-12-07 13:15:43,922 - INFO - allennlp.common.params - dataset_reader.domain_identifier = None\n",
      "2021-12-07 13:15:43,923 - INFO - allennlp.common.params - dataset_reader.bert_model_name = bert-base-uncased\n",
      "2021-12-07 13:15:43,925 - INFO - allennlp.common.params - type = bert-base-uncased\n",
      "2021-12-07 13:15:57,571 - INFO - allennlp.common.params - dataset_reader.type = srl\n",
      "2021-12-07 13:15:57,573 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
      "2021-12-07 13:15:57,579 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
      "2021-12-07 13:15:57,581 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
      "2021-12-07 13:15:57,583 - INFO - allennlp.common.params - dataset_reader.token_indexers = None\n",
      "2021-12-07 13:15:57,584 - INFO - allennlp.common.params - dataset_reader.domain_identifier = None\n",
      "2021-12-07 13:15:57,586 - INFO - allennlp.common.params - dataset_reader.bert_model_name = bert-base-uncased\n",
      "2021-12-07 13:15:57,589 - INFO - allennlp.common.params - type = bert-base-uncased\n",
      "2021-12-07 13:16:01,454 - INFO - allennlp.common.params - type = from_instances\n",
      "2021-12-07 13:16:01,455 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpewei2y34/vocabulary.\n",
      "2021-12-07 13:16:01,457 - INFO - allennlp.common.params - model.type = srl_bert\n",
      "2021-12-07 13:16:01,464 - INFO - allennlp.common.params - model.regularizer = None\n",
      "2021-12-07 13:16:01,466 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
      "2021-12-07 13:16:01,485 - INFO - allennlp.common.params - model.bert_model = bert-base-uncased\n",
      "2021-12-07 13:16:01,489 - INFO - allennlp.common.params - type = bert-base-uncased\n",
      "2021-12-07 13:16:01,494 - INFO - allennlp.common.params - type = bert-base-uncased\n",
      "2021-12-07 13:16:01,496 - INFO - allennlp.common.params - model.embedding_dropout = 0.1\n",
      "2021-12-07 13:16:01,498 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f9b021b4690>\n",
      "2021-12-07 13:16:01,504 - INFO - allennlp.common.params - model.label_smoothing = None\n",
      "2021-12-07 13:16:01,507 - INFO - allennlp.common.params - model.ignore_span_metric = False\n",
      "2021-12-07 13:16:01,509 - INFO - allennlp.common.params - model.srl_eval_path = /home/erikt/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp_models/structured_prediction/tools/srl-eval.pl\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2021-12-07 13:16:04,940 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2021-12-07 13:16:04,952 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2021-12-07 13:16:04,953 - INFO - allennlp.nn.initializers -    bert_model.embeddings.LayerNorm.bias\n",
      "2021-12-07 13:16:04,954 - INFO - allennlp.nn.initializers -    bert_model.embeddings.LayerNorm.weight\n",
      "2021-12-07 13:16:04,955 - INFO - allennlp.nn.initializers -    bert_model.embeddings.position_embeddings.weight\n",
      "2021-12-07 13:16:04,957 - INFO - allennlp.nn.initializers -    bert_model.embeddings.token_type_embeddings.weight\n",
      "2021-12-07 13:16:04,958 - INFO - allennlp.nn.initializers -    bert_model.embeddings.word_embeddings.weight\n",
      "2021-12-07 13:16:04,959 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:04,960 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:04,961 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "2021-12-07 13:16:04,962 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "2021-12-07 13:16:04,963 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.key.bias\n",
      "2021-12-07 13:16:04,965 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.key.weight\n",
      "2021-12-07 13:16:04,966 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.query.bias\n",
      "2021-12-07 13:16:04,967 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.query.weight\n",
      "2021-12-07 13:16:04,968 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.value.bias\n",
      "2021-12-07 13:16:04,970 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.attention.self.value.weight\n",
      "2021-12-07 13:16:04,971 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "2021-12-07 13:16:04,972 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "2021-12-07 13:16:04,973 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2021-12-07 13:16:04,974 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2021-12-07 13:16:04,975 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.output.dense.bias\n",
      "2021-12-07 13:16:04,976 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.0.output.dense.weight\n",
      "2021-12-07 13:16:04,981 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:04,984 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:04,986 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "2021-12-07 13:16:04,988 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "2021-12-07 13:16:04,989 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.key.bias\n",
      "2021-12-07 13:16:04,990 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.key.weight\n",
      "2021-12-07 13:16:04,991 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.query.bias\n",
      "2021-12-07 13:16:04,992 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.query.weight\n",
      "2021-12-07 13:16:04,994 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.value.bias\n",
      "2021-12-07 13:16:04,995 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.attention.self.value.weight\n",
      "2021-12-07 13:16:04,996 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "2021-12-07 13:16:04,997 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "2021-12-07 13:16:04,998 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2021-12-07 13:16:04,999 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,000 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.output.dense.bias\n",
      "2021-12-07 13:16:05,002 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.1.output.dense.weight\n",
      "2021-12-07 13:16:05,003 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,004 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,005 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "2021-12-07 13:16:05,006 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "2021-12-07 13:16:05,008 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.key.bias\n",
      "2021-12-07 13:16:05,009 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.key.weight\n",
      "2021-12-07 13:16:05,010 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.query.bias\n",
      "2021-12-07 13:16:05,011 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.query.weight\n",
      "2021-12-07 13:16:05,013 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.value.bias\n",
      "2021-12-07 13:16:05,015 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.attention.self.value.weight\n",
      "2021-12-07 13:16:05,016 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "2021-12-07 13:16:05,017 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "2021-12-07 13:16:05,018 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,019 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,020 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.output.dense.bias\n",
      "2021-12-07 13:16:05,021 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.10.output.dense.weight\n",
      "2021-12-07 13:16:05,022 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,023 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,024 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.output.dense.bias\n",
      "2021-12-07 13:16:05,025 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "2021-12-07 13:16:05,026 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.key.bias\n",
      "2021-12-07 13:16:05,027 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.key.weight\n",
      "2021-12-07 13:16:05,028 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.query.bias\n",
      "2021-12-07 13:16:05,029 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.query.weight\n",
      "2021-12-07 13:16:05,031 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.value.bias\n",
      "2021-12-07 13:16:05,032 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.attention.self.value.weight\n",
      "2021-12-07 13:16:05,033 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "2021-12-07 13:16:05,034 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "2021-12-07 13:16:05,040 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,041 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,042 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.output.dense.bias\n",
      "2021-12-07 13:16:05,043 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.11.output.dense.weight\n",
      "2021-12-07 13:16:05,045 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,046 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,047 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "2021-12-07 13:16:05,049 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "2021-12-07 13:16:05,051 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.key.bias\n",
      "2021-12-07 13:16:05,052 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.key.weight\n",
      "2021-12-07 13:16:05,053 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.query.bias\n",
      "2021-12-07 13:16:05,054 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.query.weight\n",
      "2021-12-07 13:16:05,055 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.value.bias\n",
      "2021-12-07 13:16:05,056 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.attention.self.value.weight\n",
      "2021-12-07 13:16:05,057 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "2021-12-07 13:16:05,058 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "2021-12-07 13:16:05,059 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,059 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,060 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.output.dense.bias\n",
      "2021-12-07 13:16:05,063 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.2.output.dense.weight\n",
      "2021-12-07 13:16:05,064 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,065 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,066 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "2021-12-07 13:16:05,068 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "2021-12-07 13:16:05,069 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.key.bias\n",
      "2021-12-07 13:16:05,070 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.key.weight\n",
      "2021-12-07 13:16:05,071 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.query.bias\n",
      "2021-12-07 13:16:05,072 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.query.weight\n",
      "2021-12-07 13:16:05,073 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.value.bias\n",
      "2021-12-07 13:16:05,074 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.attention.self.value.weight\n",
      "2021-12-07 13:16:05,076 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "2021-12-07 13:16:05,077 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "2021-12-07 13:16:05,078 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,079 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,081 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.output.dense.bias\n",
      "2021-12-07 13:16:05,082 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.3.output.dense.weight\n",
      "2021-12-07 13:16:05,083 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,088 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,090 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "2021-12-07 13:16:05,091 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "2021-12-07 13:16:05,091 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.key.bias\n",
      "2021-12-07 13:16:05,092 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.key.weight\n",
      "2021-12-07 13:16:05,093 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.query.bias\n",
      "2021-12-07 13:16:05,094 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.query.weight\n",
      "2021-12-07 13:16:05,095 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.value.bias\n",
      "2021-12-07 13:16:05,096 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.attention.self.value.weight\n",
      "2021-12-07 13:16:05,097 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "2021-12-07 13:16:05,098 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "2021-12-07 13:16:05,098 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,100 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,101 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.output.dense.bias\n",
      "2021-12-07 13:16:05,102 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.4.output.dense.weight\n",
      "2021-12-07 13:16:05,104 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,105 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,106 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "2021-12-07 13:16:05,108 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.output.dense.weight\n",
      "2021-12-07 13:16:05,109 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.key.bias\n",
      "2021-12-07 13:16:05,110 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.key.weight\n",
      "2021-12-07 13:16:05,111 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.query.bias\n",
      "2021-12-07 13:16:05,113 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.query.weight\n",
      "2021-12-07 13:16:05,115 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.value.bias\n",
      "2021-12-07 13:16:05,116 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.attention.self.value.weight\n",
      "2021-12-07 13:16:05,117 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "2021-12-07 13:16:05,118 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "2021-12-07 13:16:05,119 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,124 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,126 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.output.dense.bias\n",
      "2021-12-07 13:16:05,127 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.5.output.dense.weight\n",
      "2021-12-07 13:16:05,128 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,129 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,129 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "2021-12-07 13:16:05,131 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "2021-12-07 13:16:05,132 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.key.bias\n",
      "2021-12-07 13:16:05,134 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.key.weight\n",
      "2021-12-07 13:16:05,135 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.query.bias\n",
      "2021-12-07 13:16:05,136 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.query.weight\n",
      "2021-12-07 13:16:05,137 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.value.bias\n",
      "2021-12-07 13:16:05,139 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.attention.self.value.weight\n",
      "2021-12-07 13:16:05,140 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "2021-12-07 13:16:05,141 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "2021-12-07 13:16:05,142 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,143 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,146 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.output.dense.bias\n",
      "2021-12-07 13:16:05,147 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.6.output.dense.weight\n",
      "2021-12-07 13:16:05,148 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,149 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,150 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "2021-12-07 13:16:05,152 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "2021-12-07 13:16:05,153 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.key.bias\n",
      "2021-12-07 13:16:05,154 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.key.weight\n",
      "2021-12-07 13:16:05,155 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.query.bias\n",
      "2021-12-07 13:16:05,156 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.query.weight\n",
      "2021-12-07 13:16:05,157 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.value.bias\n",
      "2021-12-07 13:16:05,159 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.attention.self.value.weight\n",
      "2021-12-07 13:16:05,161 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "2021-12-07 13:16:05,162 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "2021-12-07 13:16:05,162 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,164 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,165 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.output.dense.bias\n",
      "2021-12-07 13:16:05,166 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.7.output.dense.weight\n",
      "2021-12-07 13:16:05,167 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,172 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,173 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "2021-12-07 13:16:05,174 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "2021-12-07 13:16:05,176 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.key.bias\n",
      "2021-12-07 13:16:05,177 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.key.weight\n",
      "2021-12-07 13:16:05,178 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.query.bias\n",
      "2021-12-07 13:16:05,179 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.query.weight\n",
      "2021-12-07 13:16:05,180 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.value.bias\n",
      "2021-12-07 13:16:05,181 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.attention.self.value.weight\n",
      "2021-12-07 13:16:05,182 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "2021-12-07 13:16:05,185 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "2021-12-07 13:16:05,186 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,187 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,189 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.output.dense.bias\n",
      "2021-12-07 13:16:05,190 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.8.output.dense.weight\n",
      "2021-12-07 13:16:05,191 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,192 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,193 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "2021-12-07 13:16:05,194 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "2021-12-07 13:16:05,195 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.key.bias\n",
      "2021-12-07 13:16:05,197 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.key.weight\n",
      "2021-12-07 13:16:05,198 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.query.bias\n",
      "2021-12-07 13:16:05,199 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.query.weight\n",
      "2021-12-07 13:16:05,202 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.value.bias\n",
      "2021-12-07 13:16:05,204 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.attention.self.value.weight\n",
      "2021-12-07 13:16:05,206 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "2021-12-07 13:16:05,208 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "2021-12-07 13:16:05,209 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2021-12-07 13:16:05,210 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2021-12-07 13:16:05,211 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.output.dense.bias\n",
      "2021-12-07 13:16:05,212 - INFO - allennlp.nn.initializers -    bert_model.encoder.layer.9.output.dense.weight\n",
      "2021-12-07 13:16:05,214 - INFO - allennlp.nn.initializers -    bert_model.pooler.dense.bias\n",
      "2021-12-07 13:16:05,215 - INFO - allennlp.nn.initializers -    bert_model.pooler.dense.weight\n",
      "2021-12-07 13:16:05,217 - INFO - allennlp.nn.initializers -    tag_projection_layer.bias\n",
      "2021-12-07 13:16:05,219 - INFO - allennlp.nn.initializers -    tag_projection_layer.weight\n",
      "2021-12-07 13:16:06,353 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpewei2y34\n"
     ]
    }
   ],
   "source": [
    "srl_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c920c-a5c4-4058-bb63-72c1230d0587",
   "metadata": {},
   "source": [
    "## 2. Causal relation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1a104f-cf39-456f-9434-31aed745e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022436b0-d987-4c02-9ec8-6c8b47489dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RED = 1\n",
    "BLUE = 4\n",
    "BLACK = 0\n",
    "NO_COLOR = -1\n",
    "COLOR_CONCEPT_1 = RED\n",
    "COLOR_CONCEPT_2 = BLUE\n",
    "COLOR_CONTENT_RELATION_EXPLANATION = BLACK\n",
    "CONCEPT_1 = \"Content_Concept_1\"\n",
    "CONCEPT_2 = \"Content_Concept_2\"\n",
    "CONTENT_RELATION_EXPLANATION = \"Content_Relation_Explanation\"\n",
    "NO_CONCEPT = \"\"\n",
    "COLOR_CODES = { CONCEPT_1: COLOR_CONCEPT_1, CONCEPT_2: COLOR_CONCEPT_2, CONTENT_RELATION_EXPLANATION: COLOR_CONTENT_RELATION_EXPLANATION, NO_CONCEPT: NO_COLOR }\n",
    "# 20211206 begore expansion: 34 verbs; after 78 verbs\n",
    "RELATION_VERBS = [ \"activate\", \"achieve\", \"affect\", \"aggravate\", \"allow\", \"attribute\", \"avoid\", \"base\", \"balance\", \"boost\", \n",
    "                   \"bring\", \"brought\", \"cause\", \"change\", \"compell\", \"comply\", \"compromise\", \"consolidate\", \"contain\", \"contribute\", \n",
    "                   \"control\", \"create\", \"deceive\", \"deduce\", \"depend\", \"elicit\", \"eliminate\", \"enable\", \"enact\", \"endanger\", \n",
    "                   \"enforce\", \"entail\", \"ensure\", \"erode\", \"fail\", \"flow\", \"force\", \"found\", \"increase\", \"generate\", \n",
    "                   \"ignite\", \"implicate\", \"imply\", \"induce\", \"infer\", \"influence\", \"initiate\", \"intend\", \"justify\", \"launch\", \n",
    "                   \"lead\", \"make\", \"manipulate\", \"mislead\", \"motivate\", \"obey\", \"open\", \"originate\", \"permit\", \"pick\", \n",
    "                   \"preserve\", \"produce\", \"protect\", \"provoke\", \"reduce\", \"reinforce\", \"restore\", \"result\", \"safeguard\", \"secure\", \n",
    "                   \"solve\", \"stimulate\", \"strenghten\", \"support\", \"tackle\", \"trigger\", \"undermine\", \"weaken\", ]\n",
    "REVERSE_VERBS = [ \"arise\", \"need\" ]\n",
    "ARGM_DIS_ROLES = [ \"furthermore\", \"instead\", \"moreover\", \"nevertheless\", \"otherwise\", \"therefore\", \"thus\" ]\n",
    "NO_ARG0_ROLES = [ \"he\", \"i\", \"she\", \"they\", \"we\", \"you\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb6f8c8-dcc8-4c89-93b4-78d1afab73df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tag(token):\n",
    "    if len(token) > 1:\n",
    "        if re.search(\"^\\[\", token):\n",
    "            token = re.sub(\"^\\[\", \"\", token)\n",
    "            token = re.sub(\":$\", \"\", token)\n",
    "        token = re.sub(\"\\]$\", \"\", token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33cca46-632f-4cfc-8f50-aae6f0e8fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_with_color(string, color_code):\n",
    "    print(f\"\\x1b[1;3{color_code};47m{string}\\x1b[m\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4738ff-d132-47ea-89c9-3bc4dec25543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_tag_argm_prp(argument):\n",
    "    if argument == \"ARGM-PRP\":\n",
    "        return CONCEPT_2\n",
    "    elif argument in [\"V\", \"ARG0\", \"ARG1\", \"ARG2\" ]:\n",
    "        return CONCEPT_1\n",
    "    return NO_CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f9d05f-0b73-48ce-9604-ba7283d051c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_tag_argm_dis(argument):\n",
    "    color_code = NO_COLOR\n",
    "    if argument == \"ARGM-DIS\":\n",
    "        return CONTENT_RELATION_EXPLANATION\n",
    "    elif argument != \"\":\n",
    "        return CONCEPT_2\n",
    "    return NO_CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe60a35d-1501-44d9-9aab-0bb7b150bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_tag_v(argument, roles):\n",
    "    if argument == \"V\":\n",
    "        return CONTENT_RELATION_EXPLANATION\n",
    "    elif argument in [ \"ARG0\", \"ARGM-PRD\", \"ARGM-MNR\" ] and (argument != \"ARG0\" or roles[\"ARG0\"].lower() not in NO_ARG0_ROLES):\n",
    "        return CONCEPT_1\n",
    "    elif argument in [ \"ARG1\", \"ARG2\", ]:\n",
    "        return CONCEPT_2\n",
    "    return NO_CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05549e58-5850-4ec6-a685-8470df89da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_tag_reverse_v(argument, roles):\n",
    "    if argument == \"V\":\n",
    "        return CONTENT_RELATION_EXPLANATION\n",
    "    elif argument in [ \"ARG0\", \"ARGM-PRD\", \"ARGM-MNR\" ] and (argument != \"ARG0\" or roles[\"ARG0\"].lower() not in NO_ARG0_ROLES):\n",
    "        return CONCEPT_2\n",
    "    elif argument in [ \"ARG1\", \"ARG2\", ]:\n",
    "        return CONCEPT_1\n",
    "    return NO_CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94001968-cb87-49f3-98bb-3ecc19973f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_verb(verb, RELATION_VERBS):\n",
    "    if verb in RELATION_VERBS:\n",
    "        return True\n",
    "    if re.sub(\"[ds]$\", \"\", verb) in RELATION_VERBS:\n",
    "        return True\n",
    "    if re.sub(\"ed$\", \"\", verb) in RELATION_VERBS:\n",
    "        return True\n",
    "    if re.sub(\"ing$\", \"\", verb) in RELATION_VERBS:\n",
    "        return True\n",
    "    if re.sub(\"ing$\", \"e\", verb) in RELATION_VERBS:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "355f9209-2d8f-4b6b-bd2e-ccaf448e8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token(token, argument, first_token, last_token, color_code):\n",
    "    if color_code == NO_COLOR:\n",
    "        print_token_no_color(token, argument, first_token, last_token)\n",
    "    else:\n",
    "        print_token_color(token, argument, first_token, last_token, color_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f3a494-5bb6-461b-bc0a-10e20773f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token_no_color(token, argument, first_token, last_token):\n",
    "    if first_token:\n",
    "        print(f\"[{argument} \", end=\"\")\n",
    "    print(f\"{token}\", end=\"\")\n",
    "    if last_token:\n",
    "        print(f\"]\", end=\"\")\n",
    "    print(\" \", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5782a6a-2908-434f-86c6-9a9b132765de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token_color(token, argument, first_token, last_token, color_code):\n",
    "    if first_token:\n",
    "        print_with_color(f\"[{argument} \", color_code)\n",
    "    print_with_color(f\"{token}\", color_code)\n",
    "    if last_token:\n",
    "        print_with_color(f\"]\", color_code)\n",
    "        print(\" \", end=\"\")\n",
    "    else:\n",
    "        print_with_color(\" \", color_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae3f8846-942f-45f5-8305-df86673318f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_verb(tokens, arguments, lemmas):\n",
    "    verb = \"\"\n",
    "    if len(lemmas) != len(tokens):\n",
    "        print_with_color(f\"find_verb: error: different lengths for tokens ({len(tokens)}) and lemmas ({len(lemmas)})\", 1)\n",
    "    for i in range(0, len(arguments)):\n",
    "        if arguments[i] == \"V\":\n",
    "            verb = lemmas[i]\n",
    "    return verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07e72660-d826-49ff-ac3a-40b69d19233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_analysis(analyzed_sentence):\n",
    "    tokens_in = analyzed_sentence.split()\n",
    "    arguments = []\n",
    "    tokens_out = []\n",
    "    first_tokens = []\n",
    "    last_tokens = []\n",
    "    current_argument = \"\"\n",
    "    roles = {}\n",
    "    first_token = False\n",
    "    for i in range(0, len(tokens_in)):\n",
    "        if re.search(\"^\\[\", tokens_in[i]):\n",
    "            current_argument = strip_tag(tokens_in[i])\n",
    "            first_token = True\n",
    "            continue\n",
    "        arguments.append(current_argument)\n",
    "        tokens_out.append(strip_tag(tokens_in[i]))\n",
    "        first_tokens.append(first_token)\n",
    "        last_tokens.append(False)\n",
    "        if current_argument != \"\":\n",
    "            if current_argument not in roles:\n",
    "                roles[current_argument] = strip_tag(tokens_in[i])\n",
    "            else:\n",
    "                roles[current_argument] += \" \" + strip_tag(tokens_in[i])\n",
    "        if re.search(\"\\]$\", tokens_in[i]):\n",
    "            current_argument = \"\"\n",
    "            last_tokens[-1] = True\n",
    "        first_token = False\n",
    "    return [tokens_out, arguments, first_tokens, last_tokens, roles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad4fe971-e31e-4f11-abf8-063397bb879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(sentence):\n",
    "    lemmas = []\n",
    "    results = spacy_analyze(sentence)\n",
    "    for token in results:\n",
    "        lemmas.append(token.lemma_)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31a63481-9434-43ff-9146-6a5fb0b39520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_causal_relations(analyzed_sentence, lemmas):\n",
    "    tokens, arguments, first_tokens, last_tokens, roles = convert_analysis(analyzed_sentence)\n",
    "    concepts = []\n",
    "    for i in range(0, len(tokens)):\n",
    "        if \"ARGM-PRP\" in arguments:\n",
    "            concepts.append(get_concept_tag_argm_prp(arguments[i]))\n",
    "        elif \"ARGM-DIS\" in arguments and roles[\"ARGM-DIS\"].lower() in ARGM_DIS_ROLES:\n",
    "            concepts.append(get_concept_tag_argm_dis(arguments[i]))\n",
    "        elif \"V\" in arguments and check_verb(find_verb(tokens, arguments, lemmas), RELATION_VERBS):\n",
    "            concepts.append(get_concept_tag_v(arguments[i], roles))\n",
    "        elif \"V\" in arguments and check_verb(find_verb(tokens, arguments, lemmas), REVERSE_VERBS):\n",
    "            concepts.append(get_concept_tag_reverse_v(arguments[i], roles))\n",
    "        else:\n",
    "            concepts.append(NO_CONCEPT)\n",
    "    return { \"tokens\": tokens, \"arguments\": arguments, \"first_tokens\": first_tokens, \"last_tokens\": last_tokens, \"roles\": roles, \"concepts\": concepts }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "695b907f-e7d9-42ea-b21e-e75dafa66bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(causal_relation_data):\n",
    "    for i in range(0, len(causal_relation_data[\"tokens\"])):\n",
    "        color_code = COLOR_CODES[causal_relation_data[\"concepts\"][i]]\n",
    "        print_token(causal_relation_data[\"tokens\"][i], causal_relation_data[\"arguments\"][i], causal_relation_data[\"first_tokens\"][i], causal_relation_data[\"last_tokens\"][i], color_code)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3057805e-406f-48e5-82f2-b57713e6c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def srl_analyze(sentence, filter=\"\", print_flag=True):\n",
    "    srl_analysis = srl_predictor.predict(sentence=sentence)\n",
    "    causal_relations = []\n",
    "    lemmas = get_lemmas(re.sub(\"\\s+\", \" \", sentence))\n",
    "    for verb_data in srl_analysis['verbs']:\n",
    "        causal_relations.append(find_causal_relations(verb_data['description'], lemmas))\n",
    "    if print_flag:\n",
    "        for causal_relation_data in causal_relations:\n",
    "            if filter == \"\" or re.search(filter, \" \".join(causal_relation_data[\"tokens\"] + causal_relation_data[\"arguments\"])):\n",
    "                pretty_print(causal_relation_data)\n",
    "    return causal_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c83ec-5f49-4857-bbe3-c26e1970a4e9",
   "metadata": {},
   "source": [
    "## 3. Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15844e-570e-4d1f-be62-b2f953dedbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"John sees the mountain with the snow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1011a94-4b99-445b-a402-8d8da2b2c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"You, in Greece, with our support, need to rebuild your country, your structures, your administration, \n",
    "               your economy to increase the competitiveness of Greece.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315abf68-be8c-4f3d-bf4c-2df02c86b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"This opened the way to welfare gains from stronger economic and financial integration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79c931-5910-4e5e-906c-cf0038e13294",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"One risk is the temptation for governments to overborrow because the economic costs of excessive public debt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790cf17-2704-4467-95ac-20f071abfff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"A resulting loosening of fiscal discipline in individual member states can endanger the stability-oriented monetary policy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee739b0-6890-44c8-9243-ba7232cfd06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"Therefore, the markets did not properly perform their expected policing function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a41ca-b0ba-48c7-99f3-c4475ce51df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"I hope that following the lessons of interdependence not only at global, but also at European level given by the crisis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907cf65-d7d1-4f2b-834c-c8725a789efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"European economic strategy needs the full commitment of the European political community.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c7201-cc64-4b74-ad11-4558a8e249fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"And a European economic system whose resilience flows from its single market\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72ed91-af91-4c2f-9c75-193561125851",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"there are of course considerable budgetary challenges arising from the recent exceptional measures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed79018-85a0-45fb-9d5c-2f1947602cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"And the best hope of a return to growth and job creation is inside the euro area.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da62399a-2a30-4bfd-a4eb-ce2215579c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"To conclude, let me say a few words on the euro area more generally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41d16d-acbf-4da7-9afe-0992057f9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"We have taken important, fundamental decisions over the last couple of months to safeguard the stability \n",
    "               of the euro area, and indeed we are now in the phase of implementation.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc28fe0-2fd8-40f0-bb47-e66a0dbc7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"A number of governments have embarked on a path of reform and fiscal consolidation that was unthinkable only \n",
    "               very recently, and they have taken important decisions and I encourage them to keep this determination.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608204a-2217-4403-896d-10b5b02f0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"These reforms are now being implemented and this effort must continue with credibility, with consistency, \n",
    "               with coherence over time.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df66a7-1c41-43a5-a953-c40ac915e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"As we said there will not be magic solutions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e08438-0804-4afb-a771-5f848fd6278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"We need sustained efforts and determination.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd2b3d-b1ad-4a4a-b690-6d86877ef10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"At the same time, the existing financial backstops are being used as necessary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbefe2-4e57-495d-886c-d76f4b240d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"Most recently, the financial assistance to the recapitalisation of Spanish banks has been agreed \n",
    "               and is ready for implementation.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bbeaa-a66b-4ee9-8bef-a512bca9cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"Giving to the ECB the ultimate responsibility for supervision of banks in the euro area \n",
    "               will decisively contribute to increase confidence between the banks \n",
    "               and in this way increase the financial stability in the euro area.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74993ff-629f-4444-a3e9-95d936869c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"Second, Germany is acutely aware of the need to tackle the root causes and not just the symptoms of the crisis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9f741-68e7-4ae0-b1de-5ecb8723f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"This is why it is pressing strongly for institutional reforms of the EMU framework plus structural reforms \n",
    "               and budgetary discipline in the member states.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040f38a-a5a9-4985-bec9-5407b44785a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"The introduction of the euro eliminated exchange rate risks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12883bd7-edda-4218-9303-1765e061dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"Another implication of the euro area's single monetary policy is that the key interest rates \n",
    "               are set for the currency bloc as a whole.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a3592-99cf-48eb-acdd-9cfd79fff765",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"One risk is the temptation for governments to overborrow because the economic costs of excessive public debt, \n",
    "               for example higher interest rates, can be more easily shifted to other member states.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9960a6-24ea-4ee9-b260-6112c96cd9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"Even Germany ran up excessive deficits for a few years and, even worse, championed a reform of the SGP \n",
    "               which ultimately further weakened the application of the fiscal rules.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d20c14-9a56-4456-8abb-6dd8debb1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"However, the EMU framework not only failed to avoid excessive deficits, it was also unable to prevent \n",
    "               the build-up of macroeconomic imbalances within the euro area.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931baf3e-958f-4ad6-a998-a4f8ea2cfcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"The resulting increase in domestic inflation and wages eroded the competitiveness of the countries concerned and \n",
    "               increased their dependence on capital imports.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87956f23-b8ce-48ff-8ddf-2d6ce8b5e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"The task of implementing the reforms and regaining competitiveness entailed significant political and social costs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2d27b-0dc7-48cd-9cdd-4b99aa507cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"However, these efforts, supported by a strong expansion in the global economy, allowed German growth to rebound \n",
    "               after 2005.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d13dc9-f815-4348-9384-b9a18d20e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_analyze(\"\"\"In order to achieve a turnaround and allow further assistance, it is now essential for Greece to deliver \n",
    "               on the promises that have been made.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972560a3-840f-4f4c-964f-6e7bfd49b320",
   "metadata": {},
   "source": [
    "## 4. Propbank role explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588908d7-0689-4902-b868-95fe6b5747ef",
   "metadata": {},
   "source": [
    "Modifiers in Propbank (source: http://clear.colorado.edu/compsem/documents/propbank_guidelines.pdf )\n",
    "* ADJ: Adjectival\n",
    "* ADV: Adverbials\n",
    "* CAU: Cause\n",
    "* COM: Comitative\n",
    "* DIR: Directional\n",
    "* DIS: Discourse\n",
    "* DSP: Direct Speech\n",
    "* EXT: Extent\n",
    "* GOL: Goal\n",
    "* LOC: Locative\n",
    "* LVB: Light Verb\n",
    "* MNR: Manner\n",
    "* MOD: Modal\n",
    "* NEG: Negation\n",
    "* PRD: Secondary Predication\n",
    "* PRP: Purpose\n",
    "* REC: Reciprocals\n",
    "* SLC: Relative Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674fae69-32c1-4949-989f-06bc5e5534c7",
   "metadata": {},
   "source": [
    "## 5. Process texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9179a87-35dc-42fc-b421-d4c890d5107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffdbfbeb-21d8-44be-b927-5f6c5088c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    json_data = []\n",
    "    infile = open(file_name, \"r\")\n",
    "    for line in infile:\n",
    "        json_data.append(json.loads(line))\n",
    "    infile.close()\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e89b4b4a-fcce-43da-9be3-6c2c44de0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_json(json_data_element):\n",
    "    concepts = {}\n",
    "    for label_element in json_data_element[\"label\"]:\n",
    "        label = label_element[2]\n",
    "        phrase = json_data_element[\"data\"][label_element[0]: label_element[1]]\n",
    "        if label not in concepts:\n",
    "            concepts[label] = phrase\n",
    "        else:\n",
    "            concepts[label] += \" \" + phrase\n",
    "    return list(concepts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc7c44e1-d2ff-46bc-9163-10d7072419e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_srl(srl_data):\n",
    "    concepts = {}\n",
    "    for i in range(0, len(srl_data[\"concepts\"])):\n",
    "        if srl_data[\"concepts\"][i] != NO_CONCEPT:\n",
    "            label = srl_data[\"concepts\"][i]\n",
    "            phrase = srl_data[\"tokens\"][i]\n",
    "            if label not in concepts:\n",
    "                concepts[label] = phrase\n",
    "            elif len(phrase) > 1 or re.search(\"\\w\", phrase):\n",
    "                concepts[label] += \" \" + phrase\n",
    "            else:\n",
    "                concepts[label] += phrase\n",
    "    return list(concepts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34f73384-d028-4fc3-ae63-c48fc1b4ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_precision_recall(present, found, correct, correct_phrases):\n",
    "    print(f\"found: {found}; present: {present}; correct: {correct}; \", end=\"\")\n",
    "    if found > 0:\n",
    "        print(f\"precision: {correct/found:.3f}; \", end=\"\")\n",
    "    print(f\"recall: {correct/present:.3f}\", end=\"\")\n",
    "    if len(correct_phrases) > 0:\n",
    "        print(f\"; correct phrases: {correct_phrases}\", end=\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c96d456-defd-4fbd-81cf-72dd8eef4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_concepts(concepts_json, concepts_srl):\n",
    "    correct = 0\n",
    "    concepts_srl_used = len(concepts_srl) * [False]\n",
    "    correct_phrases = []\n",
    "    for i in range(0, len(concepts_json)):\n",
    "        for j in range(0, len(concepts_srl)):\n",
    "            if not concepts_srl_used[j] and concepts_srl[j][0] == concepts_json[i][0] and concepts_srl[j][1] == concepts_json[i][1]:\n",
    "                correct += 1\n",
    "                concepts_srl_used[j] = False # change to True to avoid accepting concepts more than once \n",
    "                correct_phrases.append(concepts_srl[j][1])\n",
    "                break\n",
    "    present = len(concepts_json)\n",
    "    found = len(concepts_srl)\n",
    "    return present, found, correct, correct_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a12203b-e389-4326-b574-0324d44f4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_characters(json_data):\n",
    "    for data in json_data:\n",
    "        data[\"data\"] = re.sub(\"-\", \"_\", data[\"data\"])\n",
    "        data[\"data\"] = re.sub(\"\\[\", \"_\", data[\"data\"])\n",
    "        data[\"data\"] = re.sub(\"\\]\", \"_\", data[\"data\"])\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "08d5ee6f-2fa1-42c0-8859-1d2e2d3c0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = escape_characters(read_data(\"../data/femke-20211012.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5f44e1f-2a5c-4dbf-9270-4c63a0fa6fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Content_Concept_1', 'introduction of the euro'),\n",
       " ('Content_Relation_Explanation', 'eliminated'),\n",
       " ('Content_Concept_2', 'exchange rate risks')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_concepts_json(json_data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13f071e9-468f-4ed0-80c8-926f6cf44691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(data):\n",
    "    if type(data) != list:\n",
    "        return [data]\n",
    "    else:\n",
    "        if len(data) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            first_element_list = flatten_list(data[0])\n",
    "            rest_list = flatten_list(data[1:])\n",
    "            first_element_list.extend(rest_list)\n",
    "            return first_element_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d678ba5-9d90-4166-a631-44d385973e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_paragraph_data(paragraph_data, out_file_name=\"data.jsonl\"):\n",
    "    counter = 1\n",
    "    out_data = []\n",
    "    for paragraph in paragraph_data:\n",
    "        while True:\n",
    "            paragraph_labels = []\n",
    "            for labels in paragraph[\"labels\"]:\n",
    "                if len(labels) > 0:\n",
    "                    paragraph_labels.extend(labels[0])\n",
    "                    labels.pop(0)\n",
    "            if len(paragraph_labels) == 0:\n",
    "                break\n",
    "            out_data_item = { \"id\": counter, \"text\": paragraph[\"text\"], \"label\": paragraph_labels}\n",
    "            for key in paragraph:\n",
    "                if key not in [ \"text\", \"labels\" ]:\n",
    "                    out_data_item[key] = paragraph[key]\n",
    "            out_data.append(out_data_item)\n",
    "            counter += 1\n",
    "    out_file = open(out_file_name, \"w\")\n",
    "    for data in out_data:\n",
    "        print(json.dumps(data), file=out_file)\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bd0ef6b-ea31-4e55-aa5c-f72d177fdd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    return \" \".join([str(token) for token in spacy_analyze(string)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9aff96e7-09a8-4678-b8a1-aa394c96a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta_data(paragraph_data, json_data):\n",
    "    for paragraph in paragraph_data:\n",
    "        tokenized_paragraph = tokenize(paragraph[\"text\"])\n",
    "        text_seen = 0\n",
    "        for i in range(0, len(json_data)):\n",
    "            if tokenize(json_data[i][\"data\"]) == tokenized_paragraph:\n",
    "                text_seen += 1\n",
    "                for key in [\"paragraph_id\", \"source_id\", \"speech_id\" ]:\n",
    "                    paragraph[key] = json_data[i][key]\n",
    "                for label_data in json_data[i][\"label\"]:\n",
    "                    label = f\"[{text_seen}] \" + label_data[2]\n",
    "                    phrase = json_data[i][\"data\"][label_data[0]: label_data[1]]\n",
    "                    if label in paragraph:\n",
    "                        paragraph[label].append(phrase)\n",
    "                    else:\n",
    "                        paragraph[label] = [phrase]\n",
    "            if text_seen > 0 and tokenize(json_data[i][\"data\"]) != tokenized_paragraph:\n",
    "                break\n",
    "    return paragraph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abf60d6a-f977-4240-a677-de1c37dd339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doccano_format(srl_analysis_paragraphs, json_data):\n",
    "    paragraph_data = []\n",
    "    for srl_analysis_paragraph in srl_analysis_paragraphs:\n",
    "        text_paragraph = \"\"\n",
    "        labels_paragraph = []\n",
    "        for srl_analysis_sentence in srl_analysis_paragraph:\n",
    "            labels_sentence = []\n",
    "            text_sentence = \"\"\n",
    "            for srl_analysis in srl_analysis_sentence:\n",
    "                text = \"\"\n",
    "                labels = []\n",
    "                for i in range(0, len(srl_analysis[\"tokens\"])):\n",
    "                    token = srl_analysis[\"tokens\"][i]\n",
    "                    label = srl_analysis[\"concepts\"][i]\n",
    "                    if label != \"\":\n",
    "                        if len(labels) > 0 and labels[-1][2] == label and labels[-1][1] == len(text_paragraph) + len(text):\n",
    "                            labels[-1] = (labels[-1][0], len(text_paragraph) + len(text) + len(token) + 1, label)\n",
    "                        else:\n",
    "                            labels.append((len(text_paragraph) + len(text), len(text_paragraph) + len(text) + len(token) + 1, label))\n",
    "                    text += token + \" \"\n",
    "                if len(labels) > 0:\n",
    "                    labels_sentence.append(labels)\n",
    "                text_sentence = text\n",
    "            text_paragraph += text_sentence\n",
    "            labels_paragraph.append(labels_sentence)\n",
    "        paragraph_data.append({ \"text\": text_paragraph, \"labels\": labels_paragraph })\n",
    "    paragraph_data = add_meta_data(paragraph_data, json_data)\n",
    "    save_paragraph_data(paragraph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7a51e81-23e5-42ae-a196-57dcae0ef632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_concepts(concepts_json, paragraph_text, paragraph_id):\n",
    "    print(\"####################\")\n",
    "    concepts_srl = []\n",
    "    spacy_analysis = spacy_analyze(paragraph_text)\n",
    "    srl_analysis_paragraph = []\n",
    "    for sentence in spacy_analysis.sents:\n",
    "        srl_analysis_sentence = srl_analyze(str(sentence), filter=\"\")\n",
    "        for srl_analysis_data in srl_analysis_sentence:\n",
    "            concepts_srl.extend(get_concepts_srl(srl_analysis_data))\n",
    "        print(\"\")\n",
    "        srl_analysis_paragraph.append(srl_analysis_sentence)\n",
    "    present, found, correct, correct_phrases = evaluate_concepts(concepts_json, concepts_srl)\n",
    "    print(f\"id {paragraph_id}: \", end=\"\")\n",
    "    show_precision_recall(present, found, correct, correct_phrases)\n",
    "    return present, found, correct,srl_analysis_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58112822-ba04-44d6-88ef-b79b7cf58aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_paragraph(json_data, start_paragraph_id=0, last_paragraph_id = 100, show_only_one = True):\n",
    "    seen = {}\n",
    "    present_total = 0\n",
    "    found_total = 0\n",
    "    correct_total = 0\n",
    "    previous_paragraph_text = \"\"\n",
    "    previous_concepts_json = []\n",
    "    nbr_of_paragraphs = 0\n",
    "    srl_analysis_paragraphs = []\n",
    "    for paragraph_id in range(start_paragraph_id, last_paragraph_id):\n",
    "        new_paragraph_text = json_data[paragraph_id][\"data\"]\n",
    "        new_concepts_json = get_concepts_json(json_data[paragraph_id])\n",
    "        if new_paragraph_text in seen:\n",
    "            previous_concepts_json.extend(new_concepts_json)\n",
    "            nbr_of_paragraphs += 1\n",
    "        elif show_only_one and previous_paragraph_text != \"\":\n",
    "            break\n",
    "        else:\n",
    "            if len(previous_paragraph_text) > 0:\n",
    "                present, found, correct, srl_analysis_paragraph = process_concepts(previous_concepts_json, previous_paragraph_text, paragraph_id)\n",
    "                present_total += present\n",
    "                found_total += found\n",
    "                correct_total += correct\n",
    "                srl_analysis_paragraphs.append(srl_analysis_paragraph)\n",
    "            previous_paragraph_text = new_paragraph_text\n",
    "            previous_concepts_json = new_concepts_json\n",
    "            seen[previous_paragraph_text] = True\n",
    "            nbr_of_paragraphs = 1\n",
    "\n",
    "    present, found, correct, srl_analysis_paragraph = process_concepts(previous_concepts_json, previous_paragraph_text, paragraph_id + 1)\n",
    "    srl_analysis_paragraphs.append(srl_analysis_paragraph)\n",
    "    present_total += present\n",
    "    found_total += found\n",
    "    correct_total += correct\n",
    "    print(f\"ALL({nbr_of_paragraphs}): \", end=\"\")\n",
    "    show_precision_recall(present_total, found_total, correct_total, [])\n",
    "    save_doccano_format(srl_analysis_paragraphs, json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3451aeb-8942-4abb-8325-8f516ae12a3a",
   "metadata": {},
   "source": [
    "New paragraphs can be found on the ids: 0, 12, 14, 15, 20, 24, 25, 29, 32, 41, 47, 48, 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eaa0c392-5238-4ba5-ac23-61620ef7a825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "[ARGM-TMP Today] [ARG0 I] [V want] [ARG1 to send a clear message to the people of this great country , of Greece] . \n",
      "Today [ARG0 I] want to [V send] [ARG1 a clear message] [ARG2 to the people of this great country , of Greece] . \n",
      "\n",
      "[ARG0 I] [V know] [ARG1 that many people feel without hope] . \n",
      "I know that [ARG0 many people] [V feel] [ARG1 without hope] . \n",
      "\n",
      "Many [V are] making extremely difficult sacrifices . \n",
      "\u001b[1;31;47m[ARG0 \u001b[m\u001b[1;31;47mMany\u001b[m\u001b[1;31;47m]\u001b[m are \u001b[1;30;47m[V \u001b[m\u001b[1;30;47mmaking\u001b[m\u001b[1;30;47m]\u001b[m \u001b[1;34;47m[ARG1 \u001b[m\u001b[1;34;47mextremely\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mdifficult\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47msacrifices\u001b[m\u001b[1;34;47m]\u001b[m . \n",
      "\n",
      "[ARGM-DIS And] [ARG0 many people] [V ask] [ARG1 why they should do more] . \n",
      "And many people ask why they [V should] do more . \n",
      "And many people ask [ARGM-CAU why] [ARG0 they] [ARGM-MOD should] [V do] [ARG1 more] . \n",
      "\n",
      "[ARG0 I] [V understand] [ARG1 those concerns] . \n",
      "\n",
      "[ARGM-DIS And] [ARG0 I] [V agree] [ARG1 that some of the efforts seem unfair] . \n",
      "And I agree that some of the efforts [V seem] [ARG1 unfair] . \n",
      "\n",
      "[ARGM-DIS But] [ARG0 I] [V ask] [ARG2 people] [ARG1 to recognise the other alternatives which will be much more difficult for Greece and will affect even more the most vulnerable in the Greek society] . \n",
      "But I ask [ARG0 people] to [V recognise] [ARG1 the other alternatives which will be much more difficult for Greece and will affect even more the most vulnerable in the Greek society] . \n",
      "But I ask people to recognise the other alternatives which [V will] be much more difficult for Greece and will affect even more the most vulnerable in the Greek society . \n",
      "But I ask people to recognise [ARG1 the other alternatives] [R-ARG1 which] [ARGM-MOD will] [V be] [ARG2 much more difficult for Greece] and will affect even more the most vulnerable in the Greek society . \n",
      "But I ask people to recognise the other alternatives which will be much more difficult for Greece and [V will] affect even more the most vulnerable in the Greek society . \n",
      "But I ask people to recognise \u001b[1;31;47m[ARG0 \u001b[m\u001b[1;31;47mthe\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mother\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47malternatives\u001b[m\u001b[1;31;47m]\u001b[m [R-ARG0 which] will be much more difficult for Greece and [ARGM-MOD will] \u001b[1;30;47m[V \u001b[m\u001b[1;30;47maffect\u001b[m\u001b[1;30;47m]\u001b[m \u001b[1;34;47m[ARG1 \u001b[m\u001b[1;34;47meven\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mmore\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mthe\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mmost\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mvulnerable\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47min\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mthe\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mGreek\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47msociety\u001b[m\u001b[1;34;47m]\u001b[m . \n",
      "\n",
      "[ARGM-DIS So] [ARG1 this] [V is] [ARG2 why it is the right approach to ask Greece to reform , to increase its competitiveness to have a viable future , irrespective of the crisis] . \n",
      "So this is [ARGM-CAU why] it [V is] [ARG2 the right approach] [ARG1 to ask Greece to reform , to increase its competitiveness to have a viable future , irrespective of the crisis] . \n",
      "So this is why it is the right approach to [V ask] [ARG2 Greece] [ARG1 to reform] , to increase its competitiveness to have a viable future , irrespective of the crisis . \n",
      "So this is why it is the right approach to ask [ARG0 Greece] to [V reform] , to increase its competitiveness to have a viable future , irrespective of the crisis . \n",
      "So this is why it is the right approach to ask \u001b[1;31;47m[ARG0 \u001b[m\u001b[1;31;47mGreece\u001b[m\u001b[1;31;47m]\u001b[m to reform , to \u001b[1;31;47m[V \u001b[m\u001b[1;31;47mincrease\u001b[m\u001b[1;31;47m]\u001b[m \u001b[1;31;47m[ARG1 \u001b[m\u001b[1;31;47mits\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mcompetitiveness\u001b[m\u001b[1;31;47m]\u001b[m \u001b[1;34;47m[ARGM-PRP \u001b[m\u001b[1;34;47mto\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mhave\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47ma\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mviable\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mfuture\u001b[m\u001b[1;34;47m]\u001b[m , \u001b[1;34;47m[ARGM-PRP \u001b[m\u001b[1;34;47mirrespective\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mof\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mthe\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mcrisis\u001b[m\u001b[1;34;47m]\u001b[m . \n",
      "So this is why it is the right approach to ask [ARG0 Greece] to reform , to increase its competitiveness to [V have] [ARG1 a viable future] , [ARGM-ADV irrespective of the crisis] . \n",
      "\n",
      "[ARG0 You] , [ARGM-LOC in Greece] , \u001b[1;34;47m[ARGM-MNR \u001b[m\u001b[1;34;47mwith\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mour\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47msupport\u001b[m\u001b[1;34;47m]\u001b[m , \u001b[1;30;47m[V \u001b[m\u001b[1;30;47mneed\u001b[m\u001b[1;30;47m]\u001b[m \u001b[1;31;47m[ARG1 \u001b[m\u001b[1;31;47mto\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mrebuild\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47myour\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mcountry\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47m,\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47myour\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mstructures\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47m,\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47myour\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47madministration\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47m,\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47myour\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47meconomy\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mto\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mincrease\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mthe\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mcompetitiveness\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mof\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mGreece\u001b[m\u001b[1;31;47m]\u001b[m . \n",
      "\u001b[1;31;47m[ARG0 \u001b[m\u001b[1;31;47mYou\u001b[m\u001b[1;31;47m]\u001b[m , in Greece , with our support , need to \u001b[1;31;47m[V \u001b[m\u001b[1;31;47mrebuild\u001b[m\u001b[1;31;47m]\u001b[m \u001b[1;31;47m[ARG1 \u001b[m\u001b[1;31;47myour\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mcountry\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47m,\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47myour\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47mstructures\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47m,\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47myour\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47madministration\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47m,\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47myour\u001b[m\u001b[1;31;47m \u001b[m\u001b[1;31;47meconomy\u001b[m\u001b[1;31;47m]\u001b[m \u001b[1;34;47m[ARGM-PRP \u001b[m\u001b[1;34;47mto\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mincrease\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mthe\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mcompetitiveness\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mof\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mGreece\u001b[m\u001b[1;34;47m]\u001b[m . \n",
      "You , in Greece , with our support , need to rebuild your country , your structures , your administration , your economy to \u001b[1;30;47m[V \u001b[m\u001b[1;30;47mincrease\u001b[m\u001b[1;30;47m]\u001b[m \u001b[1;34;47m[ARG1 \u001b[m\u001b[1;34;47mthe\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mcompetitiveness\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mof\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mGreece\u001b[m\u001b[1;34;47m]\u001b[m . \n",
      "\n",
      "[ARGM-DIS And] [ARG1 the best hope of a return to growth and job creation] [V is] [ARG2 inside the euro area] . \n",
      "\n",
      "[V Staying] [ARG3 in the euro] is the best chance to avoid worse hardship and difficulties to the Greek people , namely for those in a more vulnerable position \n",
      "[ARG1 Staying in the euro] [V is] [ARG2 the best chance to avoid worse hardship and difficulties to the Greek people , namely for those in a more vulnerable position] \n",
      "Staying in the euro is the best chance to \u001b[1;30;47m[V \u001b[m\u001b[1;30;47mavoid\u001b[m\u001b[1;30;47m]\u001b[m \u001b[1;34;47m[ARG1 \u001b[m\u001b[1;34;47mworse\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mhardship\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mand\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mdifficulties\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mto\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mthe\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mGreek\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mpeople\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47m,\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mnamely\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mfor\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mthose\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47min\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47ma\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mmore\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mvulnerable\u001b[m\u001b[1;34;47m \u001b[m\u001b[1;34;47mposition\u001b[m\u001b[1;34;47m]\u001b[m \n",
      "\n",
      "id 13: found: 17; present: 3; correct: 0; precision: 0.000; recall: 0.000\n",
      "ALL(1): found: 17; present: 3; correct: 0; precision: 0.000; recall: 0.000\n"
     ]
    }
   ],
   "source": [
    "analyze_paragraph(json_data, start_paragraph_id=11, last_paragraph_id=len(json_data), show_only_one=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff502de-7348-4e64-be5d-ea00fe3e9f80",
   "metadata": {},
   "source": [
    "## 6. Visualize data\n",
    "\n",
    "Copied from: https://github.com/eriktks/cognitive_mapping/blob/main/phrases.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5f62b-9b98-4f09-afcd-c596fe74587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ids(labels, text):\n",
    "    ids = {}\n",
    "    for label_data in labels:\n",
    "        while text[label_data[0]] in \" .,?!:;\":\n",
    "            label_data[0] += 1\n",
    "        while text[label_data[1]-1] in \" .,?!:;\":\n",
    "            label_data[1] -= 1\n",
    "        if label_data[0] in ids or label_data[1] in ids:\n",
    "            print(f\"overlapping relation parts! {data_item['label']}\")\n",
    "        ids[label_data[0]] = { \"type\": \"start\", \"label\": label_data[2] }\n",
    "        ids[label_data[1]] = { \"type\":\"end\", \"label\": label_data[2] }\n",
    "    return ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f5848-7758-4690-85cb-f9a4acdcd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_colors_to_text(text, ids):\n",
    "    for key in sorted(ids.keys(), reverse=True):\n",
    "        if ids[key][\"type\"] == \"end\":\n",
    "            text = text[:key+1] + \"\\x1b[m\" + text[key+1:]\n",
    "        else:\n",
    "            if ids[key][\"label\"] == \"Content_Concept_1\":\n",
    "                color_code = 1\n",
    "            elif ids[key][\"label\"] == \"Content_Concept_2\":\n",
    "                color_code = 4\n",
    "            elif ids[key][\"label\"] == \"Content_Relation_Explanation\":\n",
    "                color_code = 0\n",
    "            else:\n",
    "                color_code = 7\n",
    "                print(f\"unknown relation part label! ({ids[key]['label']})\")\n",
    "            text = text[:key] + f\"\\x1b[1;3{color_code};47m\" + text[key:]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2720552-29c9-4e03-a77b-27fa8d793a69",
   "metadata": {},
   "source": [
    "Note that the id parameter of the function `visualize` starts at the value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86f26f-ae4e-4859-a738-42d5cfdef597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(json_data, i):\n",
    "    text = json_data[i-1][\"data\"]\n",
    "    ids = make_ids(json_data[i-1]['label'], text)\n",
    "    text = add_colors_to_text(text, ids)\n",
    "    print(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4cdb2-c9e1-49c1-8782-6ede2efd9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 2):\n",
    "    visualize(json_data, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493d02a-cc4a-41a3-bfeb-7c3c1599d455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
